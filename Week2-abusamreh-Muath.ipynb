{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a74b5e-5180-4544-86a0-b047a877eb8e",
   "metadata": {},
   "source": [
    "# Week  - Linear Regression 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5925a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.14.5)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (2.3.1)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (1.16.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (2.3.1)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c24f12c-b364-40f0-b295-7c1ba88be680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156fa14",
   "metadata": {},
   "source": [
    "First Dataset: Acute Kidney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b27e0a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"Acute Kidney.csv\" \n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "\n",
    "# Normalize columns (spaces/symbols -> underscores; lowercase)\n",
    "df.columns = (df.columns.astype(str)\n",
    "                .str.strip()\n",
    "                .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "                .str.replace(r\"[^0-9a-zA-Z_]\", \"\", regex=True)\n",
    "                .str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "734fe6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: cox_los\n"
     ]
    }
   ],
   "source": [
    "# Pick a CONTINUOUS target\n",
    "# Prefer continuous LOS; otherwise fall back to any numeric with enough unique values (not just 0/1)\n",
    "preferred = [\"cox_los\", \"los\", \"length_of_stay\"]\n",
    "target_col = next((c for c in preferred if c in df.columns), None)\n",
    "if target_col is None:\n",
    "    nums = df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "    if not nums:\n",
    "        raise ValueError(\"No numeric columns found to use as a regression target. Set target_col manually.\")\n",
    "    candidates = [c for c in nums if df[c].nunique(dropna=True) >= 10 and set(df[c].dropna().unique()) != {0,1}]\n",
    "    target_col = candidates[0] if candidates else nums[0]\n",
    "\n",
    "print(f\"Target: {target_col}\")\n",
    "y = pd.to_numeric(df[target_col], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0f28408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature typing & basic NA handling\n",
    "num_cols = df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "if target_col in num_cols:\n",
    "    num_cols.remove(target_col)\n",
    "cat_cols = df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "\n",
    "X = df[num_cols + cat_cols].copy()\n",
    "\n",
    "# Simple imputations\n",
    "for c in num_cols:\n",
    "    X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(0.0)\n",
    "for c in cat_cols:\n",
    "    X[c] = X[c].astype(\"category\").cat.add_categories([\"__missing__\"]).fillna(\"__missing__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8169b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "mask = ~y.isna()\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X[mask], y[mask], test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "526286f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess: scale numeric, OHE categoricals\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "Xtr = pre.fit_transform(X_tr)\n",
    "Xte = pre.transform(X_te)\n",
    "\n",
    "# Densify if sparse (Lasso/ElasticNet are happier with dense on older versions)\n",
    "if hasattr(Xtr, \"toarray\"):\n",
    "    Xtr = Xtr.toarray()\n",
    "    Xte = Xte.toarray()\n",
    "\n",
    "# Try to build feature names (works on newer sklearn; safe-fallback otherwise)\n",
    "def get_feature_names(preprocessor, num_cols, cat_cols):\n",
    "    names = []\n",
    "    # numeric\n",
    "    names.extend(list(num_cols))\n",
    "    # categoricals\n",
    "    try:\n",
    "        ohe = preprocessor.named_transformers_[\"cat\"]\n",
    "        try:\n",
    "            cat_names = list(ohe.get_feature_names_out(cat_cols))\n",
    "        except Exception:\n",
    "            # older versions\n",
    "            cat_names = list(ohe.get_feature_names(cat_cols))\n",
    "        names.extend(cat_names)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.array(names, dtype=object)\n",
    "\n",
    "try:\n",
    "    feat_names = get_feature_names(pre, num_cols, cat_cols)\n",
    "except Exception:\n",
    "    feat_names = None\n",
    "\n",
    "def rmse(a, b): \n",
    "    return float(np.sqrt(mean_squared_error(a, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b1f416dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate alphas (and l1_ratio), then refit\n",
    "MAX_CV_ROWS = 60_000\n",
    "if Xtr.shape[0] > MAX_CV_ROWS:\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx = rng.choice(Xtr.shape[0], size=MAX_CV_ROWS, replace=False)\n",
    "    Xcv, ycv = Xtr[idx], y_tr.iloc[idx]\n",
    "else:\n",
    "    Xcv, ycv = Xtr, y_tr\n",
    "\n",
    "alphas = np.logspace(-4, 3, 20)     # 1e-4 → 1e3\n",
    "l1_ratios = [0.15, 0.3, 0.5, 0.7, 0.85]\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5).fit(Xcv, ycv)\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=20000, random_state=42).fit(Xcv, ycv)\n",
    "enet_cv  = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios, cv=5, max_iter=30000, random_state=42).fit(Xcv, ycv)\n",
    "\n",
    "# Refit on full training data with best params\n",
    "ridge = Ridge(alpha=ridge_cv.alpha_).fit(Xtr, y_tr)\n",
    "lasso = Lasso(alpha=lasso_cv.alpha_, max_iter=20000).fit(Xtr, y_tr)\n",
    "enet  = ElasticNet(alpha=enet_cv.alpha_, l1_ratio=enet_cv.l1_ratio_, max_iter=30000).fit(Xtr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "63f5e202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Week 2 — Acute Kidney: Regularized Linear Models ===\n",
      "\n",
      "[Ridge]\n",
      "alpha=6.15848\n",
      "Train R^2: 0.9701\n",
      "Test  R^2: 0.9708\n",
      "Test  RMSE: 5.9156\n",
      "Non-zero coefficients: 63 / 63\n",
      "\n",
      "[Lasso]\n",
      "alpha=0.0885867\n",
      "Train R^2: 0.9698\n",
      "Test  R^2: 0.9713\n",
      "Test  RMSE: 5.8577\n",
      "Non-zero coefficients: 24 / 63\n",
      "\n",
      "[ElasticNet]\n",
      "alpha=0.0885867  |  l1_ratio=0.85\n",
      "Train R^2: 0.9697\n",
      "Test  R^2: 0.9711\n",
      "Test  RMSE: 5.8769\n",
      "Non-zero coefficients: 29 / 63\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "def report_model(name, model):\n",
    "    yhat_tr, yhat_te = model.predict(Xtr), model.predict(Xte)\n",
    "    print(f\"\\n[{name}]\")\n",
    "    if name == \"Ridge\":\n",
    "        print(f\"alpha={ridge_cv.alpha_:.6g}\")\n",
    "    elif name == \"Lasso\":\n",
    "        print(f\"alpha={lasso_cv.alpha_:.6g}\")\n",
    "    elif name == \"ElasticNet\":\n",
    "        print(f\"alpha={enet_cv.alpha_:.6g}  |  l1_ratio={enet_cv.l1_ratio_:.2f}\")\n",
    "    print(f\"Train R^2: {r2_score(y_tr, yhat_tr):.4f}\")\n",
    "    print(f\"Test  R^2: {r2_score(y_te, yhat_te):.4f}\")\n",
    "    print(f\"Test  RMSE: {rmse(y_te, yhat_te):.4f}\")\n",
    "    \n",
    "    # Quick sparsity view (Lasso/EN usually shrink many to zero)\n",
    "    try:\n",
    "        nnz = int(np.count_nonzero(model.coef_))\n",
    "        print(f\"Non-zero coefficients: {nnz} / {model.coef_.size}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"\\n=== Week 2 — Acute Kidney: Regularized Linear Models ===\")\n",
    "report_model(\"Ridge\", ridge)\n",
    "report_model(\"Lasso\", lasso)\n",
    "report_model(\"ElasticNet\", enet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e3e49651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Lasso coefficients (by |weight|):\n",
      "                   mort_28_day: -17.18284\n",
      "                   mort_90_day: -16.99999\n",
      "                       lactate: -0.28916\n",
      "                             p: -0.26861\n",
      "                     aki_stage: -0.21199\n",
      "                        sapsii: -0.20637\n",
      "                           ckd: +0.15719\n",
      "                           rdw: +0.15252\n",
      "                           chf: +0.14250\n",
      "                          pco2: +0.13721\n",
      "                            ne: +0.09777\n",
      "                            bp: +0.08982\n",
      "                        weight: -0.08665\n",
      "                            hb: -0.06955\n",
      "                             k: +0.05430\n"
     ]
    }
   ],
   "source": [
    "# Show top coefficients by |weight|\n",
    "def top_coefs(model, names, k=15):\n",
    "    if names is None:\n",
    "        return None\n",
    "    coefs = np.asarray(model.coef_).ravel()\n",
    "    order = np.argsort(np.abs(coefs))[::-1][:k]\n",
    "    return list(zip(names[order], coefs[order]))\n",
    "\n",
    "try:\n",
    "    print(\"\\nTop Lasso coefficients (by |weight|):\")\n",
    "    for nm, w in (top_coefs(lasso, feat_names, k=15) or []):\n",
    "        print(f\"{nm:>30s}: {w:+.5f}\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8768b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(a, b): \n",
    "    return float(np.sqrt(mean_squared_error(a, b)))\n",
    "\n",
    "def densify_if_sparse(A):\n",
    "    return A.toarray() if hasattr(A, \"toarray\") else A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bf5c21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Week 1 comparators (OLS with polynomials/interactions) \n",
    "# (A) Polynomials: degree=2 (squares + interactions) on numeric only, then concat with OHE(cats)\n",
    "poly_all = PolynomialFeatures(degree=2, include_bias=False)\n",
    "pre_poly_all = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_poly\", poly_all, num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# (B) Interactions-only\n",
    "poly_int = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "pre_poly_int = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_poly\", poly_int, num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Function to compute 5-fold CV (outer) metrics for a (preprocess -> LinearRegression) pipeline\n",
    "def cv_linear(preprocessor, X, y, n_splits=5, seed=42, tag=\"OLS\"):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    r2s, rmses = [], []\n",
    "    for tr_idx, va_idx in kf.split(X):\n",
    "        Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        Xtr_p = densify_if_sparse(preprocessor.fit_transform(Xtr))\n",
    "        Xva_p = densify_if_sparse(preprocessor.transform(Xva))\n",
    "        ols = LinearRegression()\n",
    "        ols.fit(Xtr_p, ytr)\n",
    "        yhat = ols.predict(Xva_p)\n",
    "        r2s.append(r2_score(yva, yhat))\n",
    "        rmses.append(rmse(yva, yhat))\n",
    "    return {\n",
    "        \"model\": tag,\n",
    "        \"cv_r2_mean\": float(np.mean(r2s)), \"cv_r2_std\": float(np.std(r2s)),\n",
    "        \"cv_rmse_mean\": float(np.mean(rmses)), \"cv_rmse_std\": float(np.std(rmses))\n",
    "    }\n",
    "\n",
    "cv_poly_all = cv_linear(pre_poly_all, X, y, tag=\"OLS Poly(d=2, squares+interactions)\")\n",
    "cv_poly_int = cv_linear(pre_poly_int, X, y, tag=\"OLS Interactions-only (d=2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "90afc9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 2 models (Ridge, Lasso, Elastic Net)\n",
    "# Preprocessor for regularized models: scale numeric, OHE categoricals\n",
    "pre_reg = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "alphas = np.logspace(-4, 3, 20)\n",
    "l1_ratios = [0.15, 0.3, 0.5, 0.7, 0.85]\n",
    "\n",
    "def nested_cv_regularized(X, y, model_name, n_splits=5, seed=42):\n",
    "    \"\"\"\n",
    "    Outer 5-fold CV for unbiased performance estimate.\n",
    "    Inside each outer train fold:\n",
    "      - Fit RidgeCV / LassoCV / ElasticNetCV on preprocessed data to pick hyperparams\n",
    "      - Refit on the whole outer-train and score on outer-val\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    r2s, rmses = [], []\n",
    "    chosen = []  # store chosen params per fold\n",
    "\n",
    "    for tr_idx, va_idx in kf.split(X):\n",
    "        Xtr_raw, Xva_raw = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        Xtr_p = pre_reg.fit_transform(Xtr_raw)\n",
    "        Xva_p = pre_reg.transform(Xva_raw)\n",
    "\n",
    "        # densify for Lasso/EN if needed\n",
    "        Xtr_p = densify_if_sparse(Xtr_p)\n",
    "        Xva_p = densify_if_sparse(Xva_p)\n",
    "\n",
    "        if model_name == \"Ridge\":\n",
    "            inner = RidgeCV(alphas=alphas, cv=5)\n",
    "            inner.fit(Xtr_p, ytr)\n",
    "            model = Ridge(alpha=inner.alpha_)\n",
    "            chosen.append({\"alpha\": float(inner.alpha_)})\n",
    "        elif model_name == \"Lasso\":\n",
    "            inner = LassoCV(alphas=alphas, cv=5, max_iter=20000, random_state=seed)\n",
    "            inner.fit(Xtr_p, ytr)\n",
    "            model = Lasso(alpha=inner.alpha_, max_iter=20000)\n",
    "            chosen.append({\"alpha\": float(inner.alpha_)})\n",
    "        elif model_name == \"ElasticNet\":\n",
    "            inner = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios, cv=5, max_iter=30000, random_state=seed)\n",
    "            inner.fit(Xtr_p, ytr)\n",
    "            model = ElasticNet(alpha=inner.alpha_, l1_ratio=float(inner.l1_ratio_), max_iter=30000)\n",
    "            chosen.append({\"alpha\": float(inner.alpha_), \"l1_ratio\": float(inner.l1_ratio_)})\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model_name\")\n",
    "\n",
    "        model.fit(Xtr_p, ytr)\n",
    "        yhat = model.predict(Xva_p)\n",
    "        r2s.append(r2_score(yva, yhat))\n",
    "        rmses.append(rmse(yva, yhat))\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"cv_r2_mean\": float(np.mean(r2s)), \"cv_r2_std\": float(np.std(r2s)),\n",
    "        \"cv_rmse_mean\": float(np.mean(rmses)), \"cv_rmse_std\": float(np.std(rmses)),\n",
    "        \"chosen_params_per_fold\": chosen\n",
    "    }\n",
    "\n",
    "cv_ridge = nested_cv_regularized(X, y, \"Ridge\")\n",
    "cv_lasso = nested_cv_regularized(X, y, \"Lasso\")\n",
    "cv_enet  = nested_cv_regularized(X, y, \"ElasticNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10e2d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holdout test set: head-to-head comparison \n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Week 1 holdout: OLS poly all + interactions-only\n",
    "Xtr_poly_all = densify_if_sparse(pre_poly_all.fit_transform(X_tr))\n",
    "Xte_poly_all = densify_if_sparse(pre_poly_all.transform(X_te))\n",
    "ols_poly_all = LinearRegression().fit(Xtr_poly_all, y_tr)\n",
    "yhat_tr, yhat_te = ols_poly_all.predict(Xtr_poly_all), ols_poly_all.predict(Xte_poly_all)\n",
    "hold_poly_all = {\"model\":\"OLS Poly(d=2, squares+interactions)\",\n",
    "                 \"test_r2\": r2_score(y_te, yhat_te), \"test_rmse\": rmse(y_te, yhat_te)}\n",
    "\n",
    "Xtr_poly_int = densify_if_sparse(pre_poly_int.fit_transform(X_tr))\n",
    "Xte_poly_int = densify_if_sparse(pre_poly_int.transform(X_te))\n",
    "ols_poly_int = LinearRegression().fit(Xtr_poly_int, y_tr)\n",
    "yhat_te_int = ols_poly_int.predict(Xte_poly_int)\n",
    "hold_poly_int = {\"model\":\"OLS Interactions-only (d=2)\",\n",
    "                 \"test_r2\": r2_score(y_te, yhat_te_int), \"test_rmse\": rmse(y_te, yhat_te_int)}\n",
    "\n",
    "# Week 2 holdout: choose best params on train, then refit on full train and score on test\n",
    "# Ridge\n",
    "Xtr_reg = densify_if_sparse(pre_reg.fit_transform(X_tr))\n",
    "Xte_reg = densify_if_sparse(pre_reg.transform(X_te))\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5).fit(Xtr_reg, y_tr)\n",
    "ridge = Ridge(alpha=float(ridge_cv.alpha_)).fit(Xtr_reg, y_tr)\n",
    "ridge_te = {\"model\":\"Ridge\", \"params\": {\"alpha\": float(ridge_cv.alpha_)},\n",
    "            \"test_r2\": r2_score(y_te, ridge.predict(Xte_reg)), \"test_rmse\": rmse(y_te, ridge.predict(Xte_reg))}\n",
    "\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=20000, random_state=42).fit(Xtr_reg, y_tr)\n",
    "lasso = Lasso(alpha=float(lasso_cv.alpha_), max_iter=20000).fit(Xtr_reg, y_tr)\n",
    "lasso_te = {\"model\":\"Lasso\", \"params\": {\"alpha\": float(lasso_cv.alpha_)},\n",
    "            \"test_r2\": r2_score(y_te, lasso.predict(Xte_reg)), \"test_rmse\": rmse(y_te, lasso.predict(Xte_reg))}\n",
    "\n",
    "enet_cv = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios, cv=5, max_iter=30000, random_state=42).fit(Xtr_reg, y_tr)\n",
    "enet = ElasticNet(alpha=float(enet_cv.alpha_), l1_ratio=float(enet_cv.l1_ratio_), max_iter=30000).fit(Xtr_reg, y_tr)\n",
    "enet_te = {\"model\":\"ElasticNet\", \"params\": {\"alpha\": float(enet_cv.alpha_), \"l1_ratio\": float(enet_cv.l1_ratio_)},\n",
    "           \"test_r2\": r2_score(y_te, enet.predict(Xte_reg)), \"test_rmse\": rmse(y_te, enet.predict(Xte_reg))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "07c6d2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 5-fold CV (outer) — mean ± std ===\n",
      "                              Model CV R^2 (mean ± std) CV RMSE (mean ± std)\n",
      "OLS Poly(d=2, squares+interactions)     0.8852 ± 0.0422     11.1748 ± 1.9909\n",
      "        OLS Interactions-only (d=2)     0.8984 ± 0.0298     10.5893 ± 1.4726\n",
      "                              Ridge     0.9695 ± 0.0036      5.8632 ± 0.3596\n",
      "                              Lasso     0.9699 ± 0.0037      5.8268 ± 0.3638\n",
      "                         ElasticNet     0.9697 ± 0.0038      5.8451 ± 0.3732\n",
      "\n",
      "=== Common Holdout (30%) — head-to-head ===\n",
      "                              Model  Test R^2  Test RMSE                                           Params\n",
      "OLS Poly(d=2, squares+interactions)  0.903190  10.764237                                               {}\n",
      "        OLS Interactions-only (d=2)  0.908835  10.445721                                               {}\n",
      "                              Ridge  0.970762   5.915594                    {'alpha': 6.1584821106602545}\n",
      "                              Lasso  0.971331   5.857702                   {'alpha': 0.08858667904100823}\n",
      "                         ElasticNet  0.971143   5.876914 {'alpha': 0.08858667904100823, 'l1_ratio': 0.85}\n"
     ]
    }
   ],
   "source": [
    "# Print summary tables\n",
    "def fmt_pm(mean, std, nd=4): \n",
    "    return f\"{mean:.{nd}f} ± {std:.{nd}f}\"\n",
    "\n",
    "cv_rows = [\n",
    "    cv_poly_all,\n",
    "    cv_poly_int,\n",
    "    cv_ridge,\n",
    "    cv_lasso,\n",
    "    cv_enet\n",
    "]\n",
    "cv_table = pd.DataFrame([{\n",
    "    \"Model\": r[\"model\"],\n",
    "    \"CV R^2 (mean ± std)\": fmt_pm(r[\"cv_r2_mean\"], r[\"cv_r2_std\"]),\n",
    "    \"CV RMSE (mean ± std)\": fmt_pm(r[\"cv_rmse_mean\"], r[\"cv_rmse_std\"])\n",
    "} for r in cv_rows])\n",
    "\n",
    "hold_rows = [\n",
    "    hold_poly_all,\n",
    "    hold_poly_int,\n",
    "    ridge_te,\n",
    "    lasso_te,\n",
    "    enet_te\n",
    "]\n",
    "hold_table = pd.DataFrame([{\n",
    "    \"Model\": r[\"model\"],\n",
    "    \"Test R^2\": r[\"test_r2\"],\n",
    "    \"Test RMSE\": r[\"test_rmse\"],\n",
    "    \"Params\": r.get(\"params\", {})\n",
    "} for r in hold_rows])\n",
    "\n",
    "print(\"\\n=== 5-fold CV (outer) — mean ± std ===\")\n",
    "print(cv_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Common Holdout (30%) — head-to-head ===\")\n",
    "print(hold_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16116dd0",
   "metadata": {},
   "source": [
    "Week 2 — Regularized Linear Regression (Acute Kidney)\n",
    "\n",
    "with 5-fold Cross-Validation and Week 1 OLS Comparison\n",
    "\n",
    "Objective\n",
    "\n",
    "Evaluate Ridge, Lasso, and Elastic Net on the Acute Kidney dataset with proper preprocessing, and report 5-fold CV mean ± std for R² and RMSE. Compare these models against Week 1 OLS baselines with degree-2 polynomials (squares+interactions) and interactions-only.\n",
    "\n",
    "Data & Target\n",
    "\n",
    "Dataset: Acute Kidney.csv\n",
    "\n",
    "Rows × Cols: <fill from notebook>\n",
    "\n",
    "Target (continuous): cox_los (preferred). If unavailable, we used: <target_col>\n",
    "\n",
    "Predictors: mixture of continuous (vitals, labs, severity scores) and categorical (demographics, comorbidities, clinical flags).\n",
    "\n",
    "Preprocessing\n",
    "\n",
    "Column cleanup: lower-cased, spaces/symbols → underscores.\n",
    "\n",
    "Missing values: numeric → 0.0 (quick pass); categoricals → \"__missing__\".\n",
    "\n",
    "Encoding & Scaling:\n",
    "\n",
    "Week 1 OLS: Polynomial expansion (numeric only), OHE for categoricals.\n",
    "\n",
    "Week 2 models: StandardScaler on numeric, OHE on categorical features (fit on train, transform test).\n",
    "\n",
    "Models\n",
    "Week 1 Baselines\n",
    "\n",
    "OLS Poly(d=2, squares + interactions) (numeric only) + OHE categoricals\n",
    "\n",
    "OLS Interactions-only (d=2) (numeric only) + OHE categoricals\n",
    "\n",
    "Week 2 Regularized\n",
    "\n",
    "Ridge (L2) — stabilizes coefficients under multicollinearity (shrinks, doesn’t zero).\n",
    "\n",
    "Lasso (L1) — induces sparsity / feature selection.\n",
    "\n",
    "Elastic Net (L1 + L2) — balances grouping effect and sparsity when predictors are correlated.\n",
    "\n",
    "Hyperparameter selection (nested CV):\n",
    "Inside each outer fold, we select:\n",
    "\n",
    "Ridge: alpha ∈ {1e-4 … 1e3} (logspace)\n",
    "\n",
    "Lasso: alpha ∈ {1e-4 … 1e3}, max_iter=20k\n",
    "\n",
    "Elastic Net: alpha ∈ {1e-4 … 1e3}, l1_ratio ∈ {0.15, 0.3, 0.5, 0.7, 0.85}, max_iter=30k\n",
    "\n",
    "Discussion & Comparison to Week 1\n",
    "\n",
    "Stability: Ridge/EN typically stabilize coefficients under multicollinearity introduced by polynomial terms.\n",
    "\n",
    "Sparsity: Lasso/EN zeroed <count> coefficients (from console), offering a simpler model at potentially minor cost to Test R².\n",
    "\n",
    "When to prefer which:\n",
    "\n",
    "Ridge when predictors are strongly correlated and interpretability via sparsity is less critical.\n",
    "\n",
    "Lasso/EN when you value feature selection or your signal is sparse.\n",
    "\n",
    "OLS Poly only if you can validate that the added complexity translates to robust out-of-sample gains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5673c",
   "metadata": {},
   "source": [
    "Second Dataset: Colorectal cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "231e21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & tidy\n",
    "DATA_PATH = \"colorectal_cancer_dataset.csv\" \n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "df.columns = (df.columns.astype(str)\n",
    "                .str.strip()\n",
    "                .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "                .str.replace(r\"[^0-9a-zA-Z_]\", \"\", regex=True)\n",
    "                .str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "535dab77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: age\n"
     ]
    }
   ],
   "source": [
    "# Choose a CONTINUOUS target\n",
    "# Prefer typical continuous CRC outcomes; else pick any numeric with adequate variability (not just {0,1})\n",
    "preferred = [\"survival_months\", \"time_to_event\", \"tumor_size\", \"tumor_volume\", \"age\", \"bmi\", \"los\"]\n",
    "target_col = next((c for c in preferred if c in df.columns), None)\n",
    "if target_col is None:\n",
    "    nums = df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "    if not nums:\n",
    "        raise ValueError(\"No numeric columns found for regression target. Set target_col explicitly.\")\n",
    "    candidates = [c for c in nums if df[c].nunique(dropna=True) >= 10 and set(pd.unique(df[c].dropna())) != {0,1}]\n",
    "    target_col = candidates[0] if candidates else nums[0]\n",
    "print(\"Target:\", target_col)\n",
    "\n",
    "y = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "mask = ~y.isna()\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "y = y.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# Split features\n",
    "num_cols = df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "if target_col in num_cols:\n",
    "    num_cols.remove(target_col)\n",
    "cat_cols = df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "\n",
    "# Basic NA handling for a quick pass\n",
    "X = df[num_cols + cat_cols].copy()\n",
    "for c in num_cols:\n",
    "    X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(0.0)\n",
    "for c in cat_cols:\n",
    "    X[c] = X[c].astype(\"category\").cat.add_categories([\"__missing__\"]).fillna(\"__missing__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "510957be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Week 1 comparators (OLS with polynomials/interactions) ----------------\n",
    "# (A) Polynomials: degree=2 (squares + interactions) on numeric only, then concat with OHE(cats)\n",
    "poly_all = PolynomialFeatures(degree=2, include_bias=False)\n",
    "pre_poly_all = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_poly\", poly_all, num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# (B) Interactions-only\n",
    "poly_int = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "pre_poly_int = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_poly\", poly_int, num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# 5-fold CV (outer) for OLS + polynomial preprocessors\n",
    "from sklearn.model_selection import KFold\n",
    "def cv_linear(preprocessor, X, y, n_splits=5, seed=42, tag=\"OLS\"):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    r2s, rmses = [], []\n",
    "    for tr_idx, va_idx in kf.split(X):\n",
    "        Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        Xtr_p = densify_if_sparse(preprocessor.fit_transform(Xtr))\n",
    "        Xva_p = densify_if_sparse(preprocessor.transform(Xva))\n",
    "        ols = LinearRegression()\n",
    "        ols.fit(Xtr_p, ytr)\n",
    "        yhat = ols.predict(Xva_p)\n",
    "        r2s.append(r2_score(yva, yhat))\n",
    "        rmses.append(rmse(yva, yhat))\n",
    "    return {\n",
    "        \"model\": tag,\n",
    "        \"cv_r2_mean\": float(np.mean(r2s)), \"cv_r2_std\": float(np.std(r2s)),\n",
    "        \"cv_rmse_mean\": float(np.mean(rmses)), \"cv_rmse_std\": float(np.std(rmses))\n",
    "    }\n",
    "\n",
    "cv_poly_all = cv_linear(pre_poly_all, X, y, tag=\"OLS Poly(d=2, squares+interactions)\")\n",
    "cv_poly_int = cv_linear(pre_poly_int, X, y, tag=\"OLS Interactions-only (d=2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a0fb71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Week 2 models (Ridge, Lasso, Elastic Net) with nested CV\n",
    "# Preprocessor for regularized models: scale numeric, OHE categoricals\n",
    "pre_reg = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "alphas = np.logspace(-4, 3, 20)\n",
    "l1_ratios = [0.15, 0.3, 0.5, 0.7, 0.85]\n",
    "\n",
    "def nested_cv_regularized(X, y, model_name, n_splits=5, seed=42):\n",
    "    \"\"\"\n",
    "    Outer 5-fold CV for unbiased performance estimate.\n",
    "    Inside each outer train fold:\n",
    "      - Fit RidgeCV / LassoCV / ElasticNetCV on preprocessed data to pick hyperparams\n",
    "      - Refit on the whole outer-train and score on outer-val\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    r2s, rmses, chosen = [], [], []\n",
    "\n",
    "    for tr_idx, va_idx in kf.split(X):\n",
    "        Xtr_raw, Xva_raw = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        Xtr_p = pre_reg.fit_transform(Xtr_raw)\n",
    "        Xva_p = pre_reg.transform(Xva_raw)\n",
    "        Xtr_p = densify_if_sparse(Xtr_p)\n",
    "        Xva_p = densify_if_sparse(Xva_p)\n",
    "\n",
    "        if model_name == \"Ridge\":\n",
    "            inner = RidgeCV(alphas=alphas, cv=5)\n",
    "            inner.fit(Xtr_p, ytr)\n",
    "            model = Ridge(alpha=float(inner.alpha_))\n",
    "            chosen.append({\"alpha\": float(inner.alpha_)})\n",
    "        elif model_name == \"Lasso\":\n",
    "            inner = LassoCV(alphas=alphas, cv=5, max_iter=20000, random_state=seed)\n",
    "            inner.fit(Xtr_p, ytr)\n",
    "            model = Lasso(alpha=float(inner.alpha_), max_iter=20000)\n",
    "            chosen.append({\"alpha\": float(inner.alpha_)})\n",
    "        elif model_name == \"ElasticNet\":\n",
    "            inner = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios, cv=5, max_iter=30000, random_state=seed)\n",
    "            inner.fit(Xtr_p, ytr)\n",
    "            model = ElasticNet(alpha=float(inner.alpha_), l1_ratio=float(inner.l1_ratio_), max_iter=30000)\n",
    "            chosen.append({\"alpha\": float(inner.alpha_), \"l1_ratio\": float(inner.l1_ratio_)})\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model_name\")\n",
    "\n",
    "        model.fit(Xtr_p, ytr)\n",
    "        yhat = model.predict(Xva_p)\n",
    "        r2s.append(r2_score(yva, yhat))\n",
    "        rmses.append(rmse(yva, yhat))\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"cv_r2_mean\": float(np.mean(r2s)), \"cv_r2_std\": float(np.std(r2s)),\n",
    "        \"cv_rmse_mean\": float(np.mean(rmses)), \"cv_rmse_std\": float(np.std(rmses)),\n",
    "        \"chosen_params_per_fold\": chosen\n",
    "    }\n",
    "\n",
    "cv_ridge = nested_cv_regularized(X, y, \"Ridge\")\n",
    "cv_lasso = nested_cv_regularized(X, y, \"Lasso\")\n",
    "cv_enet  = nested_cv_regularized(X, y, \"ElasticNet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "54b4312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout test set: head-to-head comparison \n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Week 1 holdout: OLS poly all + interactions-only\n",
    "Xtr_poly_all = densify_if_sparse(pre_poly_all.fit_transform(X_tr))\n",
    "Xte_poly_all = densify_if_sparse(pre_poly_all.transform(X_te))\n",
    "ols_poly_all = LinearRegression().fit(Xtr_poly_all, y_tr)\n",
    "hold_poly_all = {\n",
    "    \"model\": \"OLS Poly(d=2, squares+interactions)\",\n",
    "    \"test_r2\": r2_score(y_te, ols_poly_all.predict(Xte_poly_all)),\n",
    "    \"test_rmse\": rmse(y_te, ols_poly_all.predict(Xte_poly_all))\n",
    "}\n",
    "\n",
    "Xtr_poly_int = densify_if_sparse(pre_poly_int.fit_transform(X_tr))\n",
    "Xte_poly_int = densify_if_sparse(pre_poly_int.transform(X_te))\n",
    "ols_poly_int = LinearRegression().fit(Xtr_poly_int, y_tr)\n",
    "hold_poly_int = {\n",
    "    \"model\": \"OLS Interactions-only (d=2)\",\n",
    "    \"test_r2\": r2_score(y_te, ols_poly_int.predict(Xte_poly_int)),\n",
    "    \"test_rmse\": rmse(y_te, ols_poly_int.predict(Xte_poly_int))\n",
    "}\n",
    "\n",
    "# Week 2 holdout: choose best params on train, then refit on full train and score on test\n",
    "Xtr_reg = densify_if_sparse(pre_reg.fit_transform(X_tr))\n",
    "Xte_reg = densify_if_sparse(pre_reg.transform(X_te))\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5).fit(Xtr_reg, y_tr)\n",
    "ridge = Ridge(alpha=float(ridge_cv.alpha_)).fit(Xtr_reg, y_tr)\n",
    "ridge_te = {\"model\": \"Ridge\", \"params\": {\"alpha\": float(ridge_cv.alpha_)},\n",
    "            \"test_r2\": r2_score(y_te, ridge.predict(Xte_reg)), \"test_rmse\": rmse(y_te, ridge.predict(Xte_reg))}\n",
    "\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=20000, random_state=42).fit(Xtr_reg, y_tr)\n",
    "lasso = Lasso(alpha=float(lasso_cv.alpha_), max_iter=20000).fit(Xtr_reg, y_tr)\n",
    "lasso_te = {\"model\": \"Lasso\", \"params\": {\"alpha\": float(lasso_cv.alpha_)},\n",
    "            \"test_r2\": r2_score(y_te, lasso.predict(Xte_reg)), \"test_rmse\": rmse(y_te, lasso.predict(Xte_reg))}\n",
    "\n",
    "enet_cv = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios, cv=5, max_iter=30000, random_state=42).fit(Xtr_reg, y_tr)\n",
    "enet = ElasticNet(alpha=float(enet_cv.alpha_), l1_ratio=float(enet_cv.l1_ratio_), max_iter=30000).fit(Xtr_reg, y_tr)\n",
    "enet_te = {\"model\": \"ElasticNet\", \"params\": {\"alpha\": float(enet_cv.alpha_), \"l1_ratio\": float(enet_cv.l1_ratio_)},\n",
    "           \"test_r2\": r2_score(y_te, enet.predict(Xte_reg)), \"test_rmse\": rmse(y_te, enet.predict(Xte_reg))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "93bff081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 5-fold CV (outer) — mean ± std ===\n",
      "                              Model CV R^2 (mean ± std) CV RMSE (mean ± std)\n",
      "OLS Poly(d=2, squares+interactions)    -0.0004 ± 0.0002     11.8748 ± 0.0380\n",
      "        OLS Interactions-only (d=2)    -0.0005 ± 0.0002     11.8749 ± 0.0380\n",
      "                              Ridge    -0.0003 ± 0.0002     11.8742 ± 0.0377\n",
      "                              Lasso    -0.0001 ± 0.0001     11.8725 ± 0.0384\n",
      "                         ElasticNet    -0.0001 ± 0.0001     11.8725 ± 0.0385\n",
      "\n",
      "=== Common Holdout (30%) — head-to-head ===\n",
      "                              Model  Test R^2  Test RMSE                              Params\n",
      "OLS Poly(d=2, squares+interactions) -0.000480  11.892239                                  {}\n",
      "        OLS Interactions-only (d=2) -0.000383  11.891660                                  {}\n",
      "                              Ridge -0.000300  11.891165                   {'alpha': 1000.0}\n",
      "                              Lasso -0.000137  11.890197                   {'alpha': 1000.0}\n",
      "                         ElasticNet -0.000137  11.890197 {'alpha': 1000.0, 'l1_ratio': 0.15}\n"
     ]
    }
   ],
   "source": [
    "# Pretty print summary tables\n",
    "def fmt_pm(mean, std, nd=4):\n",
    "    return f\"{mean:.{nd}f} ± {std:.{nd}f}\"\n",
    "\n",
    "cv_rows = [cv_poly_all, cv_poly_int, cv_ridge, cv_lasso, cv_enet]\n",
    "cv_table = pd.DataFrame([{\n",
    "    \"Model\": r[\"model\"],\n",
    "    \"CV R^2 (mean ± std)\": fmt_pm(r[\"cv_r2_mean\"], r[\"cv_r2_std\"]),\n",
    "    \"CV RMSE (mean ± std)\": fmt_pm(r[\"cv_rmse_mean\"], r[\"cv_rmse_std\"])\n",
    "} for r in cv_rows])\n",
    "\n",
    "hold_rows = [hold_poly_all, hold_poly_int, ridge_te, lasso_te, enet_te]\n",
    "hold_table = pd.DataFrame([{\n",
    "    \"Model\": r[\"model\"],\n",
    "    \"Test R^2\": r[\"test_r2\"],\n",
    "    \"Test RMSE\": r[\"test_rmse\"],\n",
    "    \"Params\": r.get(\"params\", {})\n",
    "} for r in hold_rows])\n",
    "\n",
    "print(\"\\n=== 5-fold CV (outer) — mean ± std ===\")\n",
    "print(cv_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Common Holdout (30%) — head-to-head ===\")\n",
    "print(hold_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ed8d3",
   "metadata": {},
   "source": [
    "Week 2 — Regularized Linear Regression (Colorectal Cancer)\n",
    "\n",
    "with 5-fold Cross-Validation and Week 1 OLS Comparison\n",
    "\n",
    "Objective\n",
    "\n",
    "Evaluate Ridge, Lasso, and Elastic Net on the colorectal cancer dataset, report 5-fold CV mean ± std for R² and RMSE, and compare against Week 1 OLS baselines using degree-2 polynomials (squares+interactions) and interactions-only.\n",
    "\n",
    "Data & Target\n",
    "\n",
    "Dataset: colorectal_cancer_dataset.csv\n",
    "\n",
    "Rows × Cols: <fill from notebook>\n",
    "\n",
    "Target (continuous): survival_months (preferred if present). If not, used: <target_col>\n",
    "\n",
    "Predictors: mix of continuous (e.g., age, BMI, tumor size/volume, biomarkers) and categorical (e.g., sex, stage, site, therapy).\n",
    "\n",
    "Preprocessing\n",
    "\n",
    "Column cleanup: lower-cased, spaces/symbols → underscores.\n",
    "\n",
    "Missing values: numeric → 0.0 (quick pass); categoricals → \"__missing__\".\n",
    "\n",
    "Encoding & Scaling\n",
    "\n",
    "Week 1 OLS: Polynomial expansion on numeric only (degree=2); OHE for categoricals.\n",
    "\n",
    "Week 2: StandardScaler on numeric; OHE for categoricals (fit on train, transform test).\n",
    "\n",
    "Models\n",
    "Week 1 Baselines\n",
    "\n",
    "OLS Poly(d=2, squares + interactions) (numeric only) + OHE categoricals\n",
    "\n",
    "OLS Interactions-only (d=2) (numeric only) + OHE categoricals\n",
    "\n",
    "Week 2 Regularized\n",
    "\n",
    "Ridge (L2): shrinks coefficients to handle multicollinearity.\n",
    "\n",
    "Lasso (L1): induces sparsity / feature selection.\n",
    "\n",
    "Elastic Net (L1+L2): balances grouping (correlated predictors) and sparsity.\n",
    "\n",
    "Hyperparameter selection (nested CV in each outer fold):\n",
    "\n",
    "Ridge: alpha ∈ {1e-4 … 1e3} (logspace)\n",
    "\n",
    "Lasso: alpha ∈ {1e-4 … 1e3}, max_iter=20k\n",
    "\n",
    "Elastic Net: alpha ∈ {1e-4 … 1e3}, l1_ratio ∈ {0.15, 0.3, 0.5, 0.7, 0.85}, max_iter=30k\n",
    "\n",
    "Interpretation & Comparison to Week 1\n",
    "\n",
    "\n",
    "Regularization vs. Feature Expansion: Ridge/EN typically stabilize coefficients and resist overfitting from quadratic expansion; Lasso/EN may offer simpler models by zeroing weak predictors.\n",
    "\n",
    "Sparsity: Lasso/EN set many coefficients to zero (see console) → easier interpretation with minor potential trade-off in Test R²."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96124c67",
   "metadata": {},
   "source": [
    "Third Dataset: Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0fb97fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"diabetes_012_health_indicators_BRFSS2015.csv\"\n",
    "df = pd.read_csv(DATA_PATH, low_memory=True)\n",
    "\n",
    "# normalize columns\n",
    "df.columns = (df.columns.astype(str)\n",
    "                .str.strip().str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "                .str.replace(r\"[^0-9a-zA-Z_]\", \"\", regex=True)\n",
    "                .str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2d1a17be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: bmi\n"
     ]
    }
   ],
   "source": [
    "# Choose a CONTINUOUS target\n",
    "# Prefer BMI; fallback to other numeric with adequate variability (not just {0,1})\n",
    "preferred = [\"bmi\", \"menthlth\", \"physhlth\", \"genhlth\", \"age\"]\n",
    "target_col = next((c for c in preferred if c in df.columns), None)\n",
    "if target_col is None:\n",
    "    nums_all = df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "    if not nums_all:\n",
    "        raise ValueError(\"No numeric columns found for regression target. Set target_col explicitly.\")\n",
    "    candidates = [c for c in nums_all if df[c].nunique(dropna=True) >= 10 and set(pd.unique(df[c].dropna())) != {0,1}]\n",
    "    target_col = candidates[0] if candidates else nums_all[0]\n",
    "print(\"Target:\", target_col)\n",
    "\n",
    "y = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "mask = ~y.isna()\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "y = y.loc[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eee98a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature sets \n",
    "num_cols_all = df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "if target_col in num_cols_all:\n",
    "    num_cols_all.remove(target_col)\n",
    "cat_cols = df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist() \n",
    "\n",
    "# Quick NA handling (fast pass)\n",
    "X = df[num_cols_all + cat_cols].copy()\n",
    "for c in num_cols_all:\n",
    "    X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(0.0).astype(\"float32\")\n",
    "for c in cat_cols:\n",
    "    X[c] = X[c].astype(\"category\").cat.add_categories([\"__missing__\"]).fillna(\"__missing__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6b8ad2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick top-K numeric for Week 1 polynomial comparators (controls explosion)\n",
    "K_NUMERIC = 12  # raise to include more numeric in poly OLS if you have RAM\n",
    "corrs = X[num_cols_all].corrwith(y.astype(\"float32\")).abs().sort_values(ascending=False)\n",
    "num_cols_k = list(corrs.index[:min(K_NUMERIC, len(corrs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "227715ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 1 comparators (OLS with polynomials/interactions)\n",
    "# (A) Polynomials: degree=2 (squares + interactions) on top-K numeric only, then concat with OHE(cats)\n",
    "poly_all = PolynomialFeatures(degree=2, include_bias=False)\n",
    "pre_poly_all = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_poly\", poly_all, num_cols_k),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# (B) Interactions-only\n",
    "poly_int = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "pre_poly_int = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_poly\", poly_int, num_cols_k),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# 5-fold CV (outer) for OLS + polynomial preprocessors\n",
    "def cv_linear(preprocessor, X, y, n_splits=5, seed=42, tag=\"OLS\"):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    r2s, rmses = [], []\n",
    "    for tr_idx, va_idx in kf.split(X):\n",
    "        Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        Xtr_p = densify_if_sparse(preprocessor.fit_transform(Xtr))\n",
    "        Xva_p = densify_if_sparse(preprocessor.transform(Xva))\n",
    "        # cast to float32 to cut memory for large N\n",
    "        if hasattr(Xtr_p, \"astype\"):\n",
    "            Xtr_p = Xtr_p.astype(\"float32\", copy=False)\n",
    "            Xva_p = Xva_p.astype(\"float32\", copy=False)\n",
    "        ols = LinearRegression()\n",
    "        ols.fit(Xtr_p, ytr)\n",
    "        yhat = ols.predict(Xva_p)\n",
    "        r2s.append(r2_score(yva, yhat))\n",
    "        rmses.append(rmse(yva, yhat))\n",
    "    return {\n",
    "        \"model\": tag,\n",
    "        \"cv_r2_mean\": float(np.mean(r2s)), \"cv_r2_std\": float(np.std(r2s)),\n",
    "        \"cv_rmse_mean\": float(np.mean(rmses)), \"cv_rmse_std\": float(np.std(rmses))\n",
    "    }\n",
    "\n",
    "cv_poly_all = cv_linear(pre_poly_all, X, y, tag=f\"OLS Poly(d=2, squares+interactions) [top-{len(num_cols_k)} num]\")\n",
    "cv_poly_int = cv_linear(pre_poly_int, X, y, tag=f\"OLS Interactions-only (d=2) [top-{len(num_cols_k)} num]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7c7ee1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 2 models (Ridge, Lasso, Elastic Net)\n",
    "# Preprocessor for regularized models: scale numeric, OHE categoricals\n",
    "pre_reg = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols_all),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "alphas = np.logspace(-4, 3, 20)\n",
    "l1_ratios = [0.15, 0.3, 0.5, 0.7, 0.85]\n",
    "MAX_INNER_ROWS = 120_000  # cap rows during inner CV to keep speed/memory reasonable\n",
    "\n",
    "def nested_cv_regularized(X, y, model_name, n_splits=5, seed=42):\n",
    "    \"\"\"\n",
    "    Outer 5-fold CV for unbiased performance estimate.\n",
    "    Inside each outer train fold:\n",
    "      - Fit RidgeCV / LassoCV / ElasticNetCV on preprocessed data to pick hyperparams\n",
    "      - Optionally subsample rows for inner CV to keep runtime reasonable\n",
    "      - Refit on the whole outer-train and score on outer-val\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    r2s, rmses, chosen = [], [], []\n",
    "\n",
    "    for tr_idx, va_idx in kf.split(X):\n",
    "        Xtr_raw, Xva_raw = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        Xtr_p = pre_reg.fit_transform(Xtr_raw)\n",
    "        Xva_p = pre_reg.transform(Xva_raw)\n",
    "\n",
    "        Xtr_p = densify_if_sparse(Xtr_p)\n",
    "        Xva_p = densify_if_sparse(Xva_p)\n",
    "\n",
    "        # Inner CV row cap\n",
    "        if Xtr_p.shape[0] > MAX_INNER_ROWS:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            idx = rng.choice(Xtr_p.shape[0], size=MAX_INNER_ROWS, replace=False)\n",
    "            Xcv, ycv = Xtr_p[idx], ytr.iloc[idx]\n",
    "        else:\n",
    "            Xcv, ycv = Xtr_p, ytr\n",
    "\n",
    "        if model_name == \"Ridge\":\n",
    "            inner = RidgeCV(alphas=alphas, cv=5).fit(Xcv, ycv)\n",
    "            model = Ridge(alpha=float(inner.alpha_))\n",
    "            chosen.append({\"alpha\": float(inner.alpha_)})\n",
    "        elif model_name == \"Lasso\":\n",
    "            inner = LassoCV(alphas=alphas, cv=5, max_iter=20000, random_state=seed).fit(Xcv, ycv)\n",
    "            model = Lasso(alpha=float(inner.alpha_), max_iter=20000)\n",
    "            chosen.append({\"alpha\": float(inner.alpha_)})\n",
    "        elif model_name == \"ElasticNet\":\n",
    "            inner = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios, cv=5, max_iter=30000, random_state=seed).fit(Xcv, ycv)\n",
    "            model = ElasticNet(alpha=float(inner.alpha_), l1_ratio=float(inner.l1_ratio_), max_iter=30000)\n",
    "            chosen.append({\"alpha\": float(inner.alpha_), \"l1_ratio\": float(inner.l1_ratio_)})\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model_name\")\n",
    "\n",
    "        model.fit(Xtr_p, ytr)\n",
    "        yhat = model.predict(Xva_p)\n",
    "        r2s.append(r2_score(yva, yhat))\n",
    "        rmses.append(rmse(yva, yhat))\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"cv_r2_mean\": float(np.mean(r2s)), \"cv_r2_std\": float(np.std(r2s)),\n",
    "        \"cv_rmse_mean\": float(np.mean(rmses)), \"cv_rmse_std\": float(np.std(rmses)),\n",
    "        \"chosen_params_per_fold\": chosen\n",
    "    }\n",
    "\n",
    "cv_ridge = nested_cv_regularized(X, y, \"Ridge\")\n",
    "cv_lasso = nested_cv_regularized(X, y, \"Lasso\")\n",
    "cv_enet  = nested_cv_regularized(X, y, \"ElasticNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6ff5e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout test set: head-to-head comparison\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Week 1 holdout: OLS poly all + interactions-only (top-K numeric)\n",
    "Xtr_poly_all = densify_if_sparse(pre_poly_all.fit_transform(X_tr)).astype(\"float32\", copy=False)\n",
    "Xte_poly_all = densify_if_sparse(pre_poly_all.transform(X_te)).astype(\"float32\", copy=False)\n",
    "ols_poly_all = LinearRegression().fit(Xtr_poly_all, y_tr)\n",
    "hold_poly_all = {\n",
    "    \"model\": f\"OLS Poly(d=2, squares+interactions) [top-{len(num_cols_k)} num]\",\n",
    "    \"test_r2\": r2_score(y_te, ols_poly_all.predict(Xte_poly_all)),\n",
    "    \"test_rmse\": rmse(y_te, ols_poly_all.predict(Xte_poly_all))\n",
    "}\n",
    "\n",
    "Xtr_poly_int = densify_if_sparse(pre_poly_int.fit_transform(X_tr)).astype(\"float32\", copy=False)\n",
    "Xte_poly_int = densify_if_sparse(pre_poly_int.transform(X_te)).astype(\"float32\", copy=False)\n",
    "ols_poly_int = LinearRegression().fit(Xtr_poly_int, y_tr)\n",
    "hold_poly_int = {\n",
    "    \"model\": f\"OLS Interactions-only (d=2) [top-{len(num_cols_k)} num]\",\n",
    "    \"test_r2\": r2_score(y_te, ols_poly_int.predict(Xte_poly_int)),\n",
    "    \"test_rmse\": rmse(y_te, ols_poly_int.predict(Xte_poly_int))\n",
    "}\n",
    "\n",
    "# Week 2 holdout: choose best params on train, then refit on full train and score on test\n",
    "Xtr_reg = densify_if_sparse(pre_reg.fit_transform(X_tr))\n",
    "Xte_reg = densify_if_sparse(pre_reg.transform(X_te))\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5).fit(Xtr_reg, y_tr)\n",
    "ridge = Ridge(alpha=float(ridge_cv.alpha_)).fit(Xtr_reg, y_tr)\n",
    "ridge_te = {\"model\": \"Ridge\", \"params\": {\"alpha\": float(ridge_cv.alpha_)},\n",
    "            \"test_r2\": r2_score(y_te, ridge.predict(Xte_reg)), \"test_rmse\": rmse(y_te, ridge.predict(Xte_reg))}\n",
    "\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=20000, random_state=42).fit(Xtr_reg, y_tr)\n",
    "lasso = Lasso(alpha=float(lasso_cv.alpha_), max_iter=20000).fit(Xtr_reg, y_tr)\n",
    "lasso_te = {\"model\": \"Lasso\", \"params\": {\"alpha\": float(lasso_cv.alpha_)},\n",
    "            \"test_r2\": r2_score(y_te, lasso.predict(Xte_reg)), \"test_rmse\": rmse(y_te, lasso.predict(Xte_reg))}\n",
    "\n",
    "enet_cv = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios, cv=5, max_iter=30000, random_state=42).fit(Xtr_reg, y_tr)\n",
    "enet = ElasticNet(alpha=float(enet_cv.alpha_), l1_ratio=float(enet_cv.l1_ratio_), max_iter=30000).fit(Xtr_reg, y_tr)\n",
    "enet_te = {\"model\": \"ElasticNet\", \"params\": {\"alpha\": float(enet_cv.alpha_), \"l1_ratio\": float(enet_cv.l1_ratio_)},\n",
    "           \"test_r2\": r2_score(y_te, enet.predict(Xte_reg)), \"test_rmse\": rmse(y_te, enet.predict(Xte_reg))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "79016823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 5-fold CV (outer) — mean ± std ===\n",
      "                                           Model CV R^2 (mean ± std) CV RMSE (mean ± std)\n",
      "OLS Poly(d=2, squares+interactions) [top-12 num]     0.0609 ± 0.0026      6.4040 ± 0.0516\n",
      "        OLS Interactions-only (d=2) [top-12 num]     0.1080 ± 0.0036      6.2415 ± 0.0508\n",
      "                                           Ridge     0.1393 ± 0.0028      6.1308 ± 0.0486\n",
      "                                           Lasso     0.1393 ± 0.0028      6.1308 ± 0.0487\n",
      "                                      ElasticNet     0.1393 ± 0.0028      6.1308 ± 0.0487\n",
      "\n",
      "=== Common Holdout (30%) — head-to-head ===\n",
      "                                           Model  Test R^2  Test RMSE                                            Params\n",
      "OLS Poly(d=2, squares+interactions) [top-12 num]  0.095954   6.256866                                                {}\n",
      "        OLS Interactions-only (d=2) [top-12 num]  0.109788   6.208809                                                {}\n",
      "                                           Ridge  0.138154   6.109087                     {'alpha': 183.29807108324337}\n",
      "                                           Lasso  0.138168   6.109038                   {'alpha': 0.002976351441631319}\n",
      "                                      ElasticNet  0.138167   6.109043 {'alpha': 0.002976351441631319, 'l1_ratio': 0.85}\n"
     ]
    }
   ],
   "source": [
    "# Print summary tables\n",
    "def fmt_pm(mean, std, nd=4):\n",
    "    return f\"{mean:.{nd}f} ± {std:.{nd}f}\"\n",
    "\n",
    "cv_rows = [cv_poly_all, cv_poly_int, cv_ridge, cv_lasso, cv_enet]\n",
    "cv_table = pd.DataFrame([{\n",
    "    \"Model\": r[\"model\"],\n",
    "    \"CV R^2 (mean ± std)\": fmt_pm(r[\"cv_r2_mean\"], r[\"cv_r2_std\"]),\n",
    "    \"CV RMSE (mean ± std)\": fmt_pm(r[\"cv_rmse_mean\"], r[\"cv_rmse_std\"])\n",
    "} for r in cv_rows])\n",
    "\n",
    "hold_rows = [hold_poly_all, hold_poly_int, ridge_te, lasso_te, enet_te]\n",
    "hold_table = pd.DataFrame([{\n",
    "    \"Model\": r[\"model\"],\n",
    "    \"Test R^2\": r[\"test_r2\"],\n",
    "    \"Test RMSE\": r[\"test_rmse\"],\n",
    "    \"Params\": r.get(\"params\", {})\n",
    "} for r in hold_rows])\n",
    "\n",
    "print(\"\\n=== 5-fold CV (outer) — mean ± std ===\")\n",
    "print(cv_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Common Holdout (30%) — head-to-head ===\")\n",
    "print(hold_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e9dfa",
   "metadata": {},
   "source": [
    "Week 2 — Regularized Linear Regression (Diabetes BRFSS2015)\n",
    "\n",
    "with 5-fold Cross-Validation and Week 1 OLS Comparison\n",
    "\n",
    "Objective\n",
    "\n",
    "Evaluate Ridge, Lasso, and Elastic Net on the BRFSS 2015 diabetes indicators dataset. Report 5-fold CV (mean ± std) for R² and RMSE, and compare against Week 1 OLS baselines using degree-2 polynomials (squares+interactions) and interactions-only.\n",
    "\n",
    "Target (continuous): bmi\n",
    "\n",
    "Predictors: mostly continuous health indicators; sometimes categorical flags (OHE if present).\n",
    "\n",
    "Preprocessing\n",
    "\n",
    "Column cleanup: lower-cased, spaces/symbols → underscores.\n",
    "\n",
    "Missing values: numeric → 0.0; categoricals → \"__missing__\".\n",
    "\n",
    "Encoding & Scaling\n",
    "\n",
    "Week 1 OLS: Polynomial expansion on numeric only (degree=2); OHE for categoricals.\n",
    "\n",
    "Week 2: StandardScaler on numeric; OHE on categoricals (fit on train, transform test).\n",
    "\n",
    "Large dataset hygiene: float32, optional cap on polynomial features (top-K by correlation) to avoid memory blow-up.\n",
    "\n",
    "Models\n",
    "Week 1 Baselines\n",
    "\n",
    "OLS Poly(d=2, squares + interactions) (numeric only) + OHE categoricals\n",
    "\n",
    "OLS Interactions-only (d=2) (numeric only) + OHE categoricals\n",
    "\n",
    "Week 2 Regularized\n",
    "\n",
    "Ridge (L2): shrinks coefficients to handle multicollinearity.\n",
    "\n",
    "Lasso (L1): induces sparsity / feature selection.\n",
    "\n",
    "Elastic Net (L1+L2): balances grouping (correlated predictors) and sparsity.\n",
    "\n",
    "Hyperparameter selection (nested CV in each outer fold):\n",
    "\n",
    "Ridge: alpha ∈ {1e-4 … 1e3} (logspace)\n",
    "\n",
    "Lasso: alpha ∈ {1e-4 … 1e3}, max_iter=20k\n",
    "\n",
    "Elastic Net: alpha ∈ {1e-4 … 1e3}, l1_ratio ∈ {0.15, 0.3, 0.5, 0.7, 0.85}, max_iter=30k\n",
    "\n",
    "For speed on large N, inner-CV may subsample rows (e.g., ≤120k).\n",
    "\n",
    "\n",
    "\n",
    "Interpretation & Comparison to Week 1\n",
    "\n",
    "\n",
    "Regularization vs. Feature Expansion: Ridge/EN typically stabilize coefficients and resist overfitting from quadratic terms; Lasso/EN may offer simpler models by zeroing weak predictors.\n",
    "\n",
    "Sparsity: Lasso/EN set many coefficients to zero (see console) → easier interpretation with minor potential trade-off in Test R²."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
